module {
  func.func @test_net(%arg0: !torch.vtensor<[2,128,128],f32>, %arg1: !torch.vtensor<[2,128],si64>, %arg2: !torch.vtensor<[2,1,1,128],f32>) -> (!torch.vtensor<[2,128,128],f32>, !torch.vtensor<[2,32,128,128],f32>) {
    %none = torch.constant.none
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %true = torch.constant.bool true
    %0 = torch.vtensor.literal(dense<1.000000e+00> : tensor<128x4096xf32>) : !torch.vtensor<[128,4096],f32>
    %float8.838830e-02 = torch.constant.float 0.088388347648318447
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int64 = torch.constant.int 64
    %int3 = torch.constant.int 3
    %int0 = torch.constant.int 0
    %1 = torch.vtensor.literal(dense<1.000000e+00> : tensor<4096x128xf32>) : !torch.vtensor<[4096,128],f32>
    %2 = torch.vtensor.literal(dense<1.000000e+00> : tensor<4096x128xf32>) : !torch.vtensor<[4096,128],f32>
    %int1 = torch.constant.int 1
    %int-1 = torch.constant.int -1
    %int128 = torch.constant.int 128
    %int2 = torch.constant.int 2
    %3 = torch.vtensor.literal(dense<1.000000e+00> : tensor<4096x128xf32>) : !torch.vtensor<[4096,128],f32>
    %4 = torch.aten.transpose.int %3, %int0, %int1 : !torch.vtensor<[4096,128],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,4096],f32>
    %5 = torch.aten.matmul %arg0, %4 : !torch.vtensor<[2,128,128],f32>, !torch.vtensor<[128,4096],f32> -> !torch.vtensor<[2,128,4096],f32>
    %6 = torch.prim.ListConstruct %int2, %int128, %int-1, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7 = torch.aten.view %5, %6 : !torch.vtensor<[2,128,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,128,32,128],f32>
    %8 = torch.aten.transpose.int %7, %int1, %int2 : !torch.vtensor<[2,128,32,128],f32>, !torch.int, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %9 = torch.aten.transpose.int %2, %int0, %int1 : !torch.vtensor<[4096,128],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,4096],f32>
    %10 = torch.aten.matmul %arg0, %9 : !torch.vtensor<[2,128,128],f32>, !torch.vtensor<[128,4096],f32> -> !torch.vtensor<[2,128,4096],f32>
    %11 = torch.prim.ListConstruct %int2, %int128, %int-1, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %12 = torch.aten.view %10, %11 : !torch.vtensor<[2,128,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,128,32,128],f32>
    %13 = torch.aten.transpose.int %12, %int1, %int2 : !torch.vtensor<[2,128,32,128],f32>, !torch.int, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %14 = torch.aten.transpose.int %1, %int0, %int1 : !torch.vtensor<[4096,128],f32>, !torch.int, !torch.int -> !torch.vtensor<[128,4096],f32>
    %15 = torch.aten.matmul %arg0, %14 : !torch.vtensor<[2,128,128],f32>, !torch.vtensor<[128,4096],f32> -> !torch.vtensor<[2,128,4096],f32>
    %16 = torch.prim.ListConstruct %int2, %int128, %int-1, %int128 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %17 = torch.aten.view %15, %16 : !torch.vtensor<[2,128,4096],f32>, !torch.list<int> -> !torch.vtensor<[2,128,32,128],f32>
    %18 = torch.aten.transpose.int %17, %int1, %int2 : !torch.vtensor<[2,128,32,128],f32>, !torch.int, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %19 = torch.aten.slice.Tensor %arg1, %int0, %int0, %int1, %int1 : !torch.vtensor<[2,128],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,128],si64>
    %20 = torch.aten.squeeze.dim %19, %int0 : !torch.vtensor<[1,128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %21 = torch.aten.slice.Tensor %arg1, %int0, %int1, %int2, %int1 : !torch.vtensor<[2,128],si64>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,128],si64>
    %22 = torch.aten.squeeze.dim %21, %int0 : !torch.vtensor<[1,128],si64>, !torch.int -> !torch.vtensor<[128],si64>
    %23 = torch.aten.unsqueeze %20, %int1 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128,1],si64>
    %24 = torch.aten.unsqueeze %22, %int1 : !torch.vtensor<[128],si64>, !torch.int -> !torch.vtensor<[128,1],si64>
    %25 = torch.aten.mul.Tensor %8, %23 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[128,1],si64> -> !torch.vtensor<[2,32,128,128],f32>
    %26 = torch.aten.slice.Tensor %8, %int3, %int0, %int64, %int1 : !torch.vtensor<[2,32,128,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,32,128,64],f32>
    %27 = torch.aten.slice.Tensor %8, %int3, %int64, %int9223372036854775807, %int1 : !torch.vtensor<[2,32,128,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,32,128,64],f32>
    %28 = torch.aten.neg %27 : !torch.vtensor<[2,32,128,64],f32> -> !torch.vtensor<[2,32,128,64],f32>
    %29 = torch.prim.ListConstruct %28, %26 : (!torch.vtensor<[2,32,128,64],f32>, !torch.vtensor<[2,32,128,64],f32>) -> !torch.list<vtensor>
    %30 = torch.aten.cat %29, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %31 = torch.aten.mul.Tensor %30, %24 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[128,1],si64> -> !torch.vtensor<[2,32,128,128],f32>
    %32 = torch.aten.add.Tensor %25, %31, %int1 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[2,32,128,128],f32>, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %33 = torch.aten.mul.Tensor %13, %23 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[128,1],si64> -> !torch.vtensor<[2,32,128,128],f32>
    %34 = torch.aten.slice.Tensor %13, %int3, %int0, %int64, %int1 : !torch.vtensor<[2,32,128,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,32,128,64],f32>
    %35 = torch.aten.slice.Tensor %13, %int3, %int64, %int9223372036854775807, %int1 : !torch.vtensor<[2,32,128,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,32,128,64],f32>
    %36 = torch.aten.neg %35 : !torch.vtensor<[2,32,128,64],f32> -> !torch.vtensor<[2,32,128,64],f32>
    %37 = torch.prim.ListConstruct %36, %34 : (!torch.vtensor<[2,32,128,64],f32>, !torch.vtensor<[2,32,128,64],f32>) -> !torch.list<vtensor>
    %38 = torch.aten.cat %37, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %39 = torch.aten.mul.Tensor %38, %24 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[128,1],si64> -> !torch.vtensor<[2,32,128,128],f32>
    %40 = torch.aten.add.Tensor %33, %39, %int1 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[2,32,128,128],f32>, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %41 = torch.aten.transpose.int %40, %int2, %int3 : !torch.vtensor<[2,32,128,128],f32>, !torch.int, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %42 = torch.aten.matmul %32, %41 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[2,32,128,128],f32> -> !torch.vtensor<[2,32,128,128],f32>
    %43 = torch.aten.mul.Scalar %42, %float8.838830e-02 : !torch.vtensor<[2,32,128,128],f32>, !torch.float -> !torch.vtensor<[2,32,128,128],f32>
    %44 = torch.aten.add.Tensor %43, %arg2, %int1 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[2,1,1,128],f32>, !torch.int -> !torch.vtensor<[2,32,128,128],f32>
    %values, %indices = torch.aten.max.dim %44, %int-1, %true : !torch.vtensor<[2,32,128,128],f32>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,128,1],f32>, !torch.vtensor<[2,32,128,1],si64>
    %45 = torch.aten.sub.Tensor %44, %values, %float1.000000e00 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[2,32,128,1],f32>, !torch.float -> !torch.vtensor<[2,32,128,128],f32>
    %46 = torch.aten.exp %45 : !torch.vtensor<[2,32,128,128],f32> -> !torch.vtensor<[2,32,128,128],f32>
    %47 = torch.prim.ListConstruct %int-1 : (!torch.int) -> !torch.list<int>
    %48 = torch.aten.sum.dim_IntList %46, %47, %true, %none : !torch.vtensor<[2,32,128,128],f32>, !torch.list<int>, !torch.bool, !torch.none -> !torch.vtensor<[2,32,128,1],f32>
    %49 = torch.aten.div.Tensor %46, %48 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[2,32,128,1],f32> -> !torch.vtensor<[2,32,128,128],f32>
    %50 = torch.aten.matmul %49, %18 : !torch.vtensor<[2,32,128,128],f32>, !torch.vtensor<[2,32,128,128],f32> -> !torch.vtensor<[2,32,128,128],f32>
    %51 = torch.aten.transpose.int %50, %int1, %int2 : !torch.vtensor<[2,32,128,128],f32>, !torch.int, !torch.int -> !torch.vtensor<[2,128,32,128],f32>
    %52 = torch.prim.ListConstruct %int2, %int128, %int-1 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %53 = torch.aten.view %51, %52 : !torch.vtensor<[2,128,32,128],f32>, !torch.list<int> -> !torch.vtensor<[2,128,4096],f32>
    %54 = torch.aten.transpose.int %0, %int0, %int1 : !torch.vtensor<[128,4096],f32>, !torch.int, !torch.int -> !torch.vtensor<[4096,128],f32>
    %55 = torch.aten.matmul %53, %54 : !torch.vtensor<[2,128,4096],f32>, !torch.vtensor<[4096,128],f32> -> !torch.vtensor<[2,128,128],f32>
    return %55, %49 : !torch.vtensor<[2,128,128],f32>, !torch.vtensor<[2,32,128,128],f32>
  }
}
