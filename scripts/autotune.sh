deepspeed --autotuning tune --num_nodes=1 --num_gpus=1 ~/transformers/examples/pytorch/language-modeling/run_clm.py --deepspeed ds_config.json --model_name_or_path "meta-llama/Llama-3.2-1B-Instruct" --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1  --do_train --do_eval --fp16 --per_device_train_batch_size 4 --gradient_accumulation_steps 8 --learning_rate 2e-5 --num_train_epochs 10 --output_dir ./out --overwrite_output_dir
